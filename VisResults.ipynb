{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IM6CZzW_CH0"
   },
   "source": [
    "# Stockformer Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5GFng7v7Eq0"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# if not 'Informer2020' in sys.path:\n",
    "#     sys.path += ['Informer2020']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIjZdN5e_SWe"
   },
   "source": [
    "## Open log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPdt-Kwc_RRZ"
   },
   "outputs": [],
   "source": [
    "from utils.tools import dotdict\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils.ipynb_helpers import (\n",
    "    args_from_setting,\n",
    "    setting_from_args,\n",
    "    handle_gpu,\n",
    "    read_data,\n",
    ")\n",
    "import yaml\n",
    "\n",
    "log_dir = \"lightning_logs/stockformer_custom_ftMS_sl16_ll0_pl1_ei9_diNone_co1_iTrue_dm128_nh8_el12_dlNone_df2048_atprob_fc5_ebNone_dtFalse_mxFalse_full_1h_0/version_3\"\n",
    "with open(os.path.join(log_dir, \"hparams.yaml\"), \"r\") as file:\n",
    "    args = dotdict(yaml.load(file, Loader=yaml.FullLoader))\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNhEP_7sAgqC"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMRk8VkQ2Iko",
    "outputId": "bbf3cd10-7294-472d-e330-21e00f20963a"
   },
   "outputs": [],
   "source": [
    "# When we finished exp.train(setting) and exp.test(setting), we will get a trained model and the results of test experiment\n",
    "# The results of test experiment will be saved in ./results/{setting}/pred.npy (prediction of test dataset) and ./results/{setting}/true.npy (groundtruth of test dataset)\n",
    "\n",
    "tp_dict = {}\n",
    "for flag in [\"train\", \"val\", \"test\"]:\n",
    "    device = 0\n",
    "    while True: # Device Loop\n",
    "        preds_path = os.path.join(log_dir, f\"results/pred_{flag}_{device}.npy\")\n",
    "        trues_path = os.path.join(log_dir, f\"results/true_{flag}_{device}.npy\")\n",
    "        dates_path = os.path.join(log_dir, f\"results/date_{flag}_{device}.npy\")\n",
    "        if (\n",
    "            os.path.exists(preds_path)\n",
    "            and os.path.exists(trues_path)\n",
    "            and os.path.exists(dates_path)\n",
    "        ):\n",
    "            dp = [np.load(trues_path), np.load(preds_path), np.load(dates_path)]\n",
    "            tp_dict[flag] = dp if flag not in tp_dict else [np.append(tpdfi, dpi,axis=0) for tpdfi, dpi in zip(tp_dict[flag], dp)]\n",
    "            s = np.argsort(tp_dict[flag][2], axis=None)\n",
    "            tp_dict[flag] = list(map(lambda x: x[s], tp_dict[flag]))\n",
    "        else:\n",
    "            # Done searching for devices\n",
    "            break\n",
    "        device+=1\n",
    "\n",
    "\n",
    "print(\"Open true/pred data for:\", list(tp_dict.keys()))\n",
    "\n",
    "# [samples, pred_len, dimensions]\n",
    "print(\n",
    "    tp_dict[\"train\"][0].shape, tp_dict[\"val\"][0].shape, tp_dict[\"test\"][0].shape, \"\\n\\n\"\n",
    ")\n",
    "\n",
    "for flag in tp_dict:\n",
    "    trues, preds, dates = tp_dict[flag]\n",
    "    print(\n",
    "        f\"{flag}\\ttrues.shape: {trues.shape}, preds.shape: {preds.shape}, dates.shape: {preds.shape}\"\n",
    "    )\n",
    "\n",
    "    MSE = np.square(np.subtract(trues, preds)).mean()\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    print(\"against preds\", MSE, RMSE)\n",
    "\n",
    "    MSE = np.square(np.subtract(trues, np.zeros(preds.shape))).mean()\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    print(\"against 0s\", MSE, RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "kyPuOPGAAjl3",
    "outputId": "8554f6f8-c13a-43e1-b04b-5f27823445d0"
   },
   "outputs": [],
   "source": [
    "# draw OT prediction\n",
    "for flag in tp_dict:\n",
    "    trues, preds, dates = tp_dict[flag]\n",
    "    true = trues[:, 0, 0]\n",
    "    pred = preds[:, 0, 0]\n",
    "    date = dates[:, 0]\n",
    "    plt.figure(num=flag, figsize=(16, 4))\n",
    "    plt.title(flag)\n",
    "    plt.plot(date, true, label=\"GroundTruth\", linestyle=\"\", marker=\".\", markersize=4)\n",
    "    plt.plot(date, pred, label=\"Prediction\", linestyle=\"\", marker=\".\", markersize=4)\n",
    "    plt.plot(date, np.zeros(date.shape), color=\"red\")\n",
    "    # plt.scatter(range(trues.shape[0]), trues[:,0,0], marker='v', color='r', label='GroundTruth')\n",
    "    # plt.scatter(range(trues.shape[0]), preds[:,0,0], marker='^', color='m', label='Prediction')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(num=flag, figsize=(16, 4))\n",
    "    plt.title(\"Diff histogram\")\n",
    "    # plt.hist(np.abs(true), bins=len(true)//6, label='Diff 0', alpha=0.5)\n",
    "    # plt.hist(np.abs(true - pred), bins=len(true)//6, label='Diff Pred', alpha=0.5)\n",
    "    plt.hist(\n",
    "        [np.abs(true), np.abs(true - pred)], bins=60, label=[\"Diff 0\", \"Diff Pred\"]\n",
    "    )\n",
    "    plt.xlabel(\"Diff Value\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # df = pd.concat([pd.DataFrame(a, columns=[f\"{i}\"]) for i, a in enumerate([np.abs(true - pred), np.abs(true)])], axis=1)\n",
    "\n",
    "    # # plot the data\n",
    "    # df.plot.hist(stacked=True, bins=len(true), density=True, figsize=(10, 6), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic back-test based on buying in predicted direction if prediction is above a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tracker = (0, 0)\n",
    "\n",
    "# Tracks results\n",
    "tracker = {}\n",
    "\n",
    "df = read_data(os.path.join(args.root_path, args.data_path))\n",
    "\n",
    "# Get the percentile to check thresh until\n",
    "percentile = [50, 0.0]\n",
    "for flag in [\"train\"]:  # tp_dict:\n",
    "    _, preds, _ = tp_dict[flag]\n",
    "    percentile[1] += np.percentile(\n",
    "        np.abs(preds), percentile[0]\n",
    "    )  # np.median(np.abs(preds))\n",
    "percentile[1] /= len(tp_dict)\n",
    "print(f\"{percentile[0]}'th percentile: {percentile[1]}\")\n",
    "\n",
    "ticker, field = args.target.split(\"_\")\n",
    "assert field == \"pctchange\"\n",
    "\n",
    "for thresh in [.0002]:#np.linspace(0, 0.00025, 501):\n",
    "    # print(\"thresh:\", thresh)\n",
    "    tracker[thresh] = {}\n",
    "    track = {}\n",
    "    for flag in tp_dict:\n",
    "        trues, preds, dates = tp_dict[flag]\n",
    "        # trues, preds = np.exp(trues), np.exp(preds)\n",
    "        true = trues[:, 0, 0].copy()\n",
    "        pred = preds[:, 0, 0].copy()\n",
    "        date = pd.DatetimeIndex(dates[:, 0], tz=\"UTC\")\n",
    "\n",
    "        df_flag = df.loc[date][np.abs(pred) >= thresh]\n",
    "\n",
    "        # Filter by thresh. Note in log scale\n",
    "        true_c_log = true[np.abs(pred) >= thresh]\n",
    "        pred_c_log = pred[np.abs(pred) >= thresh]\n",
    "\n",
    "        # Percent direction correct, ie up or down\n",
    "        pct_dir_correct = np.sum(np.sign(true_c_log) == np.sign(pred_c_log)) / len(\n",
    "            true_c_log\n",
    "        )\n",
    "\n",
    "        true_c, pred_c = np.exp(true_c_log), np.exp(pred_c_log)\n",
    "\n",
    "        # # Turn pct_change to price change\n",
    "        # true_price_change = df_flag[ticker][\"open\"] * (true_c-1)\n",
    "        # pred_price_change = df_flag[ticker][\"open\"] * (pred_c-1)\n",
    "        # # Profit if you always bought one share with shorting\n",
    "        # p_one_share_wshort = (true_price_change * np.sign(pred_price_change)).sum()\n",
    "        # # Profit if you always bought one share without shorting\n",
    "        # p_one_share = (true_price_change * np.sign(pred_price_change))[pred_price_change > 0].sum()\n",
    "\n",
    "        # Important: Percent profit with & without shorting\n",
    "        # pct_profit_wshort = ((true_c-1) * np.sign(pred_c-1) + 1).prod()\n",
    "        pct_profit_wshort = np.exp((true_c_log * np.sign(pred_c_log)).sum())\n",
    "        # pct_profit = ((true_c-1) * np.sign(pred_c-1) + 1)[pred_c > 1].prod()\n",
    "        pct_profit = np.exp((true_c_log * np.sign(pred_c_log))[pred_c_log > 0].sum())\n",
    "\n",
    "        # Important: percent profit with & without shorting with partial purchase\n",
    "        pct_profit_tanh_wshort = np.exp((true_c_log * np.tanh(1000 * pred_c_log)).sum())\n",
    "        pct_profit_tanh = np.exp(\n",
    "            (true_c_log * np.tanh(1000 * pred_c_log))[pred_c_log > 0].sum()\n",
    "        )\n",
    "\n",
    "        # Optimal percent profit without shorting\n",
    "        # pct_profit_opt = ((true_c-1) * np.sign(true_c-1) + 1)[true_c > 1].prod()\n",
    "        pct_profit_opt = np.exp(\n",
    "            (true_c_log * np.sign(true_c_log))[true_c_log > 0].sum()\n",
    "        )\n",
    "\n",
    "        # Tune threshhold based off of train's metric we care about\n",
    "        tune_metric = pct_profit_tanh if args.loss == \"stock_tanh\" else pct_profit\n",
    "        if tune_metric > max_tracker[0] and flag == \"train\":\n",
    "            max_tracker = (tune_metric, thresh)\n",
    "\n",
    "        # Save\n",
    "        tracker[thresh][flag] = {\n",
    "            \"pct_profit\": pct_profit,\n",
    "            \"pct_profit_wshort\": pct_profit_wshort,\n",
    "            # \"p_one_share\": p_one_share, \"p_one_share_wshort\": p_one_share_wshort,\n",
    "            \"pct_profit_tanh\": pct_profit_tanh,\n",
    "            \"pct_profit_tanh_wshort\": pct_profit_tanh_wshort,\n",
    "            \"pct_excluded\": (len(pred) - len(pred_c_log[pred_c_log > 0])) / len(pred),\n",
    "            \"pct_excluded_wshort\": (len(pred) - len(pred_c_log)) / len(pred),\n",
    "            \"pct_dir_correct\": pct_dir_correct,\n",
    "            \"pct_profit_opt\": pct_profit_opt,\n",
    "        }\n",
    "\n",
    "\n",
    "best_thresh = max_tracker[1]\n",
    "print(\"best thresh:\", best_thresh)\n",
    "for k in tracker[best_thresh]:\n",
    "    print(f\"{k}\\t\", tracker[best_thresh][k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1, sharex=True, figsize=(16, 8))\n",
    "best_thresh=.0002\n",
    "for flag in tp_dict:\n",
    "    trues, preds, dates = tp_dict[flag]\n",
    "    true = trues[:, 0, 0].copy()\n",
    "    pred = preds[:, 0, 0].copy()\n",
    "    date = pd.DatetimeIndex(dates[:, 0], tz=\"UTC\")\n",
    "\n",
    "    # Filter by best_thresh. Note in log scale\n",
    "    true_c_log = true[np.abs(pred) >= best_thresh]\n",
    "    pred_c_log = pred[np.abs(pred) >= best_thresh]\n",
    "    date_c = date[np.abs(pred) >= best_thresh]\n",
    "\n",
    "\n",
    "    pct_profit_wshort = np.exp((true_c_log * np.sign(pred_c_log)).sum())\n",
    "    axs[0].plot(date_c, np.exp(np.cumsum((true_c_log * np.sign(pred_c_log)))), label=flag)\n",
    "    axs[0].set_ylabel(\"pct_profit_wshort\")\n",
    "    axs[0].set_title(\"pct_profit_wshort\")\n",
    "    axs[0].grid(axis = 'y')\n",
    "\n",
    "    pct_profit = np.exp((true_c_log * np.sign(pred_c_log))[pred_c_log > 0].sum())\n",
    "    axs[1].plot(date_c[pred_c_log > 0], np.exp(np.cumsum((true_c_log * np.sign(pred_c_log))[pred_c_log > 0])))#, label=flag)\n",
    "    axs[1].set_ylabel(\"pct_profit\")\n",
    "    axs[1].set_title(\"pct_profit\")\n",
    "    axs[1].grid(axis = 'y')\n",
    "\n",
    "fig.legend()\n",
    "fig.suptitle(\"Cumulative metrics overtime\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iflTTl0quCoK",
    "outputId": "3708fc91-517e-4c83-e133-059381bde271"
   },
   "outputs": [],
   "source": [
    "# args.output_attention = True\n",
    "\n",
    "# exp = Exp(args)\n",
    "\n",
    "# model = exp.model\n",
    "\n",
    "# path = os.path.join(args.checkpoints, setting, \"checkpoint.pth\")\n",
    "\n",
    "# print(model.load_state_dict(torch.load(path)))\n",
    "\n",
    "# df = pd.read_csv(os.path.join(args.root_path, args.data_path))\n",
    "# df[args.cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDdzqm9HAk2C"
   },
   "outputs": [],
   "source": [
    "# from data_provider.data_loader import Dataset_Custom\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# Data = Dataset_Custom\n",
    "# timeenc = 0 if args.embed != \"timeF\" else 1\n",
    "# flag = \"test\"\n",
    "# shuffle_flag = False\n",
    "# drop_last = True\n",
    "# batch_size = 1\n",
    "# data_set = Data(args, flag=flag)\n",
    "\n",
    "# data_loader = DataLoader(\n",
    "#     data_set,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=shuffle_flag,\n",
    "#     num_workers=args.num_workers,\n",
    "#     drop_last=drop_last,\n",
    "# )\n",
    "\n",
    "\n",
    "# idx = 0\n",
    "# for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, ds_index) in enumerate(\n",
    "#     data_loader\n",
    "# ):\n",
    "#     if i != idx:\n",
    "#         continue\n",
    "#     batch_x = batch_x.float().to(exp.device)\n",
    "#     batch_y = batch_y.float()\n",
    "\n",
    "#     batch_x_mark = batch_x_mark.float().to(exp.device)\n",
    "#     batch_y_mark = batch_y_mark.float().to(exp.device)\n",
    "\n",
    "#     dec_inp = torch.zeros_like(batch_y[:, -args.pred_len :, :]).float()\n",
    "#     dec_inp = (\n",
    "#         torch.cat([batch_y[:, : args.label_len, :], dec_inp], dim=1)\n",
    "#         .float()\n",
    "#         .to(exp.device)\n",
    "#     )\n",
    "\n",
    "#     outputs, attn = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "\n",
    "# print(attn[0].shape, attn[1].shape)  # , attn[2].shape\n",
    "\n",
    "\n",
    "# layers = [0, 1]\n",
    "# distil = \"Distil\" if args.distil else \"NoDistil\"\n",
    "# for layer in layers:\n",
    "#     print(\"\\n\\n==========================\")\n",
    "#     print(\"Showing attention layer\", layer)\n",
    "#     print(\"==========================\\n\\n\")\n",
    "#     for h in range(0, args.n_heads):\n",
    "#         plt.figure(figsize=[10, 8])\n",
    "#         plt.title(f\"Informer, {distil}, attn:{args.attn} layer:{layer} head:{h}\")\n",
    "#         A = attn[layer][0, h].detach().cpu().numpy()\n",
    "#         ax = sns.heatmap(A, vmin=0, vmax=A.max() + 0.01)\n",
    "#         plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "af42076dbd0dc4bad15096163af3221a89f84ba4155674ba19b5bb3ffd4e804c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
