{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IM6CZzW_CH0"
   },
   "source": [
    "# Stockformer Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5GFng7v7Eq0"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# if not 'Informer2020' in sys.path:\n",
    "#     sys.path += ['Informer2020']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIjZdN5e_SWe"
   },
   "source": [
    "## Open log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPdt-Kwc_RRZ"
   },
   "outputs": [],
   "source": [
    "from utils.tools import dotdict\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.ipynb_helpers import (\n",
    "    args_from_setting,\n",
    "    setting_from_args,\n",
    "    handle_gpu,\n",
    "    read_data,\n",
    ")\n",
    "import yaml\n",
    "from utils.stock_metrics import (\n",
    "    apply_threshold_metric,\n",
    "    PctProfitDirection,\n",
    "    PctProfitTanh,\n",
    "    LogPctProfitDirection,\n",
    "    LogPctProfitTanh,\n",
    "    pct_direction,\n",
    ")\n",
    "log_dir = \"lightning_logs/stockformer_custom_ftMS_sl16_ll0_pl1_ei9_diNone_co1_iNone_dm512_nh512_el12_dlNone_df2048_atfull_fc5_ebNone_dtFalse_mxFalse_full_1h_0/version_1\"\n",
    "# log_dir = \"lightning_logs/stockformer_custom_ftMS_sl16_ll0_pl1_ei9_diNone_co1_iNone_dm256_nh256_el12_dlNone_df2048_atfull_fc5_ebNone_dtFalse_mxFalse_full_1h_0/version_11\"\n",
    "# log_dir = \"lightning_logs/stockformer_custom_ftMS_sl16_ll0_pl1_ei9_diNone_co1_iNone_dm128_nh128_el12_dlNone_df2048_atfull_fc5_ebNone_dtFalse_mxFalse_full_1h_0/version_4\"\n",
    "# log_dir = \"lightning_logs/stockformer_custom_ftMS_sl16_ll0_pl1_ei9_diNone_co1_iNone_dm1028_nh512_el12_dlNone_df2048_atfull_fc5_ebNone_dtFalse_mxFalse_full_1h_0/version_0\"\n",
    "# log_dir = \"lightning_logs/stockformer_custom_ftMS_sl16_ll0_pl1_ei9_diNone_co1_iNone_dm512_nh512_el12_dlNone_df2048_atprob_fc5_ebNone_dtFalse_mxFalse_full_1h_0/version_25\"\n",
    "# log_dir = \"lightning_logs/stockformer_custom_ftMS_sl16_ll0_pl1_ei9_diNone_co1_iNone_dm256_nh256_el12_dlNone_df2048_atprob_fc5_ebNone_dtFalse_mxFalse_full_1h_0/version_0\"\n",
    "log_dir = \"lightning_logs/stockformer_custom_ftMS_sl16_ll0_pl1_ei9_diNone_co1_iNone_dm512_nh512_el12_dlNone_df2048_atfull_fc5_ebNone_dtFalse_mxFalse_full_1h_0/version_9\"\n",
    "with open(os.path.join(log_dir, \"hparams.yaml\"), \"r\") as file:\n",
    "    args = dotdict(yaml.load(file, Loader=yaml.FullLoader))\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data(os.path.join(args.root_path,args.data_path))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNhEP_7sAgqC"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMRk8VkQ2Iko",
    "outputId": "bbf3cd10-7294-472d-e330-21e00f20963a"
   },
   "outputs": [],
   "source": [
    "# When we finished exp.train(setting) and exp.test(setting), we will get a trained model and the results of test experiment\n",
    "# The results of test experiment will be saved in ./results/{setting}/pred.npy (prediction of test dataset) and ./results/{setting}/true.npy (groundtruth of test dataset)\n",
    "\n",
    "tp_dict = {}\n",
    "for flag in [\"train\", \"val\", \"test\"]:\n",
    "    device = 0\n",
    "    while True: # Device Loop\n",
    "        preds_path = os.path.join(log_dir, f\"results/pred_{flag}_{device}.npy\")\n",
    "        trues_path = os.path.join(log_dir, f\"results/true_{flag}_{device}.npy\")\n",
    "        dates_path = os.path.join(log_dir, f\"results/date_{flag}_{device}.npy\")\n",
    "        if (\n",
    "            os.path.exists(preds_path)\n",
    "            and os.path.exists(trues_path)\n",
    "            and os.path.exists(dates_path)\n",
    "        ):\n",
    "            dp = [np.load(trues_path)[:,0,0], np.load(preds_path)[:,0,0], np.load(dates_path)[:,0]]\n",
    "            tp_dict[flag] = dp if flag not in tp_dict else [np.append(tpdfi, dpi,axis=0) for tpdfi, dpi in zip(tp_dict[flag], dp)]\n",
    "            s = np.argsort(tp_dict[flag][2], axis=None)\n",
    "            tp_dict[flag] = list(map(lambda x: x[s], tp_dict[flag]))\n",
    "\n",
    "            tp_dict[flag][2] = pd.DatetimeIndex(tp_dict[flag][2], tz=\"UTC\")\n",
    "\n",
    "            # Override trues with df target data\n",
    "            if not(\"mse\" in args.loss and not args.inverse_output):\n",
    "                print(\"OVERRIDING trues with df target \")\n",
    "                df_flag = df.loc[tp_dict[flag][2]]\n",
    "                t = args.target.split(\"_\")\n",
    "                df_target = df_flag[t[0]][t[1]].to_numpy()\n",
    "                tp_dict[flag][0] = df_target\n",
    "\n",
    "        else:\n",
    "            # Done searching for devices\n",
    "            break\n",
    "        device+=1\n",
    "\n",
    "\n",
    "print(\"Open true/pred data for:\", list(tp_dict.keys()))\n",
    "\n",
    "# [samples, pred_len, dimensions]\n",
    "print(\n",
    "    tp_dict[\"train\"][0].shape, tp_dict[\"val\"][0].shape, tp_dict[\"test\"][0].shape, \"\\n\\n\"\n",
    ")\n",
    "\n",
    "for flag in tp_dict:\n",
    "    trues, preds, dates = tp_dict[flag]\n",
    "    print(\n",
    "        f\"{flag}\\ttrues.shape: {trues.shape}, preds.shape: {preds.shape}, dates.shape: {dates.shape}\"\n",
    "    )\n",
    "\n",
    "    MSE = np.square(np.subtract(trues, preds)).mean()\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    print(\"against preds\", MSE, RMSE)\n",
    "\n",
    "    MSE = np.square(np.subtract(trues, np.zeros(preds.shape))).mean()\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    print(\"against 0s\", MSE, RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "kyPuOPGAAjl3",
    "outputId": "8554f6f8-c13a-43e1-b04b-5f27823445d0"
   },
   "outputs": [],
   "source": [
    "# draw OT prediction\n",
    "for flag in tp_dict:\n",
    "    true, pred, date = tp_dict[flag]\n",
    "\n",
    "    if \"stock\" in args.loss:\n",
    "        true = true / np.linalg.norm(true)\n",
    "        pred = pred / np.linalg.norm(pred)\n",
    "\n",
    "    # pred = pred.copy()*10 #* (np.random.random(pred.shape)-.5)*100\n",
    "    plt.figure(num=flag, figsize=(16, 4))\n",
    "    plt.title(flag)\n",
    "    plt.plot(date, true, label=\"GroundTruth\", linestyle=\"\", marker=\".\", markersize=4)\n",
    "    plt.plot(date, pred, label=\"Prediction\", linestyle=\"\", marker=\".\", markersize=4)\n",
    "    plt.plot(date, np.zeros(date.shape), color=\"red\")\n",
    "    # plt.scatter(range(trues.shape[0]), trues[:,0,0], marker='v', color='r', label='GroundTruth')\n",
    "    # plt.scatter(range(trues.shape[0]), preds[:,0,0], marker='^', color='m', label='Prediction')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(num=flag, figsize=(16, 4))\n",
    "    plt.title(\"Diff histogram\")\n",
    "    # plt.hist(np.abs(true), bins=len(true)//6, label='Diff 0', alpha=0.5)\n",
    "    # plt.hist(np.abs(true - pred), bins=len(true)//6, label='Diff Pred', alpha=0.5)\n",
    "    plt.hist(\n",
    "        [np.abs(true), np.abs(true - pred)], bins=60, label=[\"Diff 0\", \"Diff Pred\"]\n",
    "    )\n",
    "    plt.xlabel(\"Diff Value\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # df = pd.concat([pd.DataFrame(a, columns=[f\"{i}\"]) for i, a in enumerate([np.abs(true - pred), np.abs(true)])], axis=1)\n",
    "\n",
    "    # # plot the data\n",
    "    # df.plot.hist(stacked=True, bins=len(true), density=True, figsize=(10, 6), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic back-test based on buying in predicted direction if prediction is above a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tracker = (0, 0)\n",
    "\n",
    "# Tracks results\n",
    "tracker = {}\n",
    "\n",
    "df = read_data(os.path.join(args.root_path, args.data_path))\n",
    "\n",
    "# Get the percentile to check thresh until\n",
    "percentile = [50, 0.0]\n",
    "for flag in [\"train\"]:  # tp_dict:\n",
    "    _, preds, _ = tp_dict[flag]\n",
    "    percentile[1] += np.percentile(\n",
    "        np.abs(preds), percentile[0]\n",
    "    ) \n",
    "    print(np.median(np.abs(preds)))\n",
    "# percentile[1] /= len(tp_dict)\n",
    "print(f\"{percentile[0]}'th percentile: {percentile[1]}\")\n",
    "\n",
    "ticker, field = args.target.split(\"_\")\n",
    "assert field == \"pctchange\" or field == \"logpctchange\"\n",
    "\n",
    "for thresh in np.linspace(0, percentile[1], 501):\n",
    "    # print(\"thresh:\", thresh)\n",
    "    tracker[thresh] = {}\n",
    "    track = {}\n",
    "    for flag in tp_dict:\n",
    "        true, pred, date = tp_dict[flag]\n",
    "\n",
    "        # Filter by thresh. Note in log scale\n",
    "        pred_f, true_f = apply_threshold_metric(pred, true, thresh)\n",
    "        df_f = df.loc[date[np.abs(pred) >= thresh]]\n",
    "\n",
    "        # Percent direction correct, ie up or down\n",
    "        pct_dir_correct = pct_direction(pred_f, true_f)\n",
    "\n",
    "        # Percent profit all in\n",
    "        pct_profit_dir = LogPctProfitDirection.metric(pred_f, true_f, short_filter=None)\n",
    "        pct_profit_dir_nshort = LogPctProfitDirection.metric(pred_f, true_f, short_filter=\"ns\")\n",
    "        pct_profit_dir_oshort = LogPctProfitDirection.metric(pred_f, true_f, short_filter=\"os\")\n",
    "\n",
    "        # Percent profit with tanh partial purchase\n",
    "        pct_profit_tanh = LogPctProfitTanh.metric(pred_f, true_f, short_filter=None)\n",
    "        pct_profit_tanh_nshort = LogPctProfitTanh.metric(pred_f, true_f, short_filter=\"ns\")\n",
    "        pct_profit_tanh_oshort = LogPctProfitTanh.metric(pred_f, true_f, short_filter=\"os\")\n",
    "\n",
    "        # Optimal percent profit\n",
    "        pct_profit_dir_opt = LogPctProfitDirection.metric(true_f, true_f)\n",
    "\n",
    "        # Tune threshhold based off of train's metric we care about\n",
    "        tune_metric = pct_profit_tanh if args.loss == \"stock_tanh\" else pct_profit_dir\n",
    "        if tune_metric > max_tracker[0] and flag == \"train\":\n",
    "            max_tracker = (tune_metric, thresh)\n",
    "\n",
    "        # Save\n",
    "        tracker[thresh][flag] = {\n",
    "            \"pct_profit_dir\": pct_profit_dir,\n",
    "            \"pct_profit_dir_nshort\": pct_profit_dir_nshort,\n",
    "            \"pct_profit_dir_oshort\": pct_profit_dir_oshort,\n",
    "            \"pct_profit_tanh\": pct_profit_tanh,\n",
    "            \"pct_profit_tanh_nshort\": pct_profit_tanh_nshort,\n",
    "            \"pct_profit_tanh_oshort\": pct_profit_tanh_oshort,\n",
    "            \"pct_excluded\": (len(pred) - len(pred_f)) / len(pred), \n",
    "            \"pct_excluded_nshort\": (len(pred) - len(pred_f[pred_f > 0])) / len(pred),\n",
    "            \"pct_excluded_oshort\": (len(pred) - len(pred_f[pred_f < 0])) / len(pred),\n",
    "            \"pct_dir_correct\": pct_dir_correct,\n",
    "            \"pct_profit_dir_opt\": pct_profit_dir_opt,\n",
    "        }\n",
    "\n",
    "\n",
    "best_thresh = max_tracker[1]\n",
    "print(\"best thresh:\", best_thresh)\n",
    "for data_group in tracker[best_thresh]:\n",
    "    print(data_group, end=\"\\t\") \n",
    "    pprint(tracker[best_thresh][data_group], indent=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,1, sharex=True, figsize=(16, 8))\n",
    "\n",
    "for flag in [\"train\", \"val\", \"test\"]: #tp_dict:\n",
    "    true, pred, date = tp_dict[flag]\n",
    "\n",
    "    # if flag == \"train\":\n",
    "    #     true = true[:1000]\n",
    "    #     pred = pred[:1000]\n",
    "    #     date = date[:1000]\n",
    "\n",
    "\n",
    "    # Filter by best_thresh. Note in log scale\n",
    "    pred_f, true_f = apply_threshold_metric(pred, true, best_thresh)\n",
    "    date_f = date[np.abs(pred) >= best_thresh]\n",
    "    df_f = df.loc[date[np.abs(pred) >= best_thresh]]\n",
    "\n",
    "    if \"lpp\" in args.loss:\n",
    "        metric = LogPctProfitDirection\n",
    "        metric_name = \"pct_profit_dir\"\n",
    "    elif \"tanh\" in args.loss:\n",
    "        metric = LogPctProfitTanh\n",
    "        metric_name = \"pct_profit_tanh\"\n",
    "    elif \"mse\" in args.loss:\n",
    "        metric = LogPctProfitDirection\n",
    "        metric_name = \"pct_profit_dir\"\n",
    "    elif \"mae\" in args.loss:\n",
    "        metric = LogPctProfitDirection\n",
    "        metric_name = \"pct_profit_dir\"\n",
    "\n",
    "    axs[0].plot(date_f, metric.accumulate(pred_f, true_f, short_filter=None), label=flag)\n",
    "    axs[0].set_ylabel(metric_name)\n",
    "    axs[0].set_title(metric_name)\n",
    "    axs[0].grid(axis = 'y')\n",
    "\n",
    "    axs[1].plot(date_f[pred_f > 0], metric.accumulate(pred_f, true_f, short_filter=\"ns\"))#, label=flag)\n",
    "    axs[1].set_ylabel(f\"{metric_name}_nshort\")\n",
    "    axs[1].set_title(f\"{metric_name}_nshort\")\n",
    "    axs[1].grid(axis = 'y')\n",
    "\n",
    "    axs[2].plot(date_f[pred_f < 0], metric.accumulate(pred_f, true_f, short_filter=\"os\"))#, label=flag)\n",
    "    axs[2].set_ylabel(f\"{metric_name}_oshort\")\n",
    "    axs[2].set_title(f\"{metric_name}_oshort\")\n",
    "    axs[2].grid(axis = 'y')\n",
    "\n",
    "    axs[3].plot(date_f, np.exp(np.cumsum(true_f)), label=\"Market\")\n",
    "    # axs[3].set_ylabel(\"Market\")\n",
    "    axs[3].set_title(\"Market\")\n",
    "    axs[3].grid(axis = 'y')\n",
    "\n",
    "fig.legend()\n",
    "fig.suptitle(\"Cumulative metrics overtime\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iflTTl0quCoK",
    "outputId": "3708fc91-517e-4c83-e133-059381bde271"
   },
   "outputs": [],
   "source": [
    "# args.output_attention = True\n",
    "\n",
    "# exp = Exp(args)\n",
    "\n",
    "# model = exp.model\n",
    "\n",
    "# path = os.path.join(args.checkpoints, setting, \"checkpoint.pth\")\n",
    "\n",
    "# print(model.load_state_dict(torch.load(path)))\n",
    "\n",
    "# df = pd.read_csv(os.path.join(args.root_path, args.data_path))\n",
    "# df[args.cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDdzqm9HAk2C"
   },
   "outputs": [],
   "source": [
    "# from data_provider.data_loader import Dataset_Custom\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# Data = Dataset_Custom\n",
    "# timeenc = 0 if args.embed != \"timeF\" else 1\n",
    "# flag = \"test\"\n",
    "# shuffle_flag = False\n",
    "# drop_last = True\n",
    "# batch_size = 1\n",
    "# data_set = Data(args, flag=flag)\n",
    "\n",
    "# data_loader = DataLoader(\n",
    "#     data_set,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=shuffle_flag,\n",
    "#     num_workers=args.num_workers,\n",
    "#     drop_last=drop_last,\n",
    "# )\n",
    "\n",
    "\n",
    "# idx = 0\n",
    "# for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, ds_index) in enumerate(\n",
    "#     data_loader\n",
    "# ):\n",
    "#     if i != idx:\n",
    "#         continue\n",
    "#     batch_x = batch_x.float().to(exp.device)\n",
    "#     batch_y = batch_y.float()\n",
    "\n",
    "#     batch_x_mark = batch_x_mark.float().to(exp.device)\n",
    "#     batch_y_mark = batch_y_mark.float().to(exp.device)\n",
    "\n",
    "#     dec_inp = torch.zeros_like(batch_y[:, -args.pred_len :, :]).float()\n",
    "#     dec_inp = (\n",
    "#         torch.cat([batch_y[:, : args.label_len, :], dec_inp], dim=1)\n",
    "#         .float()\n",
    "#         .to(exp.device)\n",
    "#     )\n",
    "\n",
    "#     outputs, attn = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "\n",
    "# print(attn[0].shape, attn[1].shape)  # , attn[2].shape\n",
    "\n",
    "\n",
    "# layers = [0, 1]\n",
    "# distil = \"Distil\" if args.distil else \"NoDistil\"\n",
    "# for layer in layers:\n",
    "#     print(\"\\n\\n==========================\")\n",
    "#     print(\"Showing attention layer\", layer)\n",
    "#     print(\"==========================\\n\\n\")\n",
    "#     for h in range(0, args.n_heads):\n",
    "#         plt.figure(figsize=[10, 8])\n",
    "#         plt.title(f\"Informer, {distil}, attn:{args.attn} layer:{layer} head:{h}\")\n",
    "#         A = attn[layer][0, h].detach().cpu().numpy()\n",
    "#         ax = sns.heatmap(A, vmin=0, vmax=A.max() + 0.01)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from exp.exp_timeseries import ExpTimeseries\n",
    "from data_provider.data_module import CustomDataModule\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",devices=1)#, log_dir=os.path.abspath(log_dir))\n",
    "\n",
    "exp = ExpTimeseries.load_from_checkpoint(\n",
    "    os.path.join(log_dir, \"checkpoints/checkpoint.ckpt\"), config=args\n",
    ")\n",
    "data_module = CustomDataModule(args, 0)\n",
    "\n",
    "# Test Model\n",
    "# t = trainer.test(exp, data_module)\n",
    "\n",
    "# # Predict and Save Results\n",
    "results = trainer.predict(exp, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "informer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed7196bae4639695404bed277a4eeb420fe0fa6732153dda2ee017a5c94f627e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
